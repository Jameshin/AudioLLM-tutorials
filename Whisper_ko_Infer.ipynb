{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-22 15:22:41.926976: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-22 15:22:41.973812: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-22 15:22:42.707138: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ed627bec6d645299f86c5276f45d28a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/2.26k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efc9134dd2504345b00e454547dad6ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/967M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fb60dea703e43b89097d1eed2f43869",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/3.86k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e71d3c9a02ee45648bc9229afdcb9694",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/283k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fa3a2d1d4a44ea0bb330faa22c1817e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a94df10add0644dab3804758d2133be8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/494k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e935f8f623454f328515c3fd517bdebf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "normalizer.json:   0%|          | 0.00/52.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb069397dee64eb08d83326352948c4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/34.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b938c6ebb6b46fabfc5764751c622e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b766c1afe40f4bf9b88c264fe8a12b00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/339 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"automatic-speech-recognition\", model=\"Eveready/whisper-small-ko\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_speech(filepath):\n",
    "    output = pipe(\n",
    "        filepath,\n",
    "        max_new_tokens=256,\n",
    "        generate_kwargs={\n",
    "            \"task\": \"transcribe\",\n",
    "            \"language\": \"korean\",\n",
    "        },  # update with the language you've fine-tuned on\n",
    "        chunk_length_s=30,\n",
    "        batch_size=8,\n",
    "    )\n",
    "    return output[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shandy77/.local/lib/python3.10/site-packages/gradio/outputs.py:22: UserWarning: Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "demo = gr.Blocks()\n",
    "\n",
    "mic_transcribe = gr.Interface(\n",
    "    fn=transcribe_speech,\n",
    "    inputs=gr.Audio(source=\"microphone\", streaming=True),\n",
    "    outputs=gr.outputs.Textbox(),\n",
    ")\n",
    "\n",
    "file_transcribe = gr.Interface(\n",
    "    fn=transcribe_speech,\n",
    "    inputs=gr.Audio(source=\"upload\", type=\"filepath\"),\n",
    "    outputs=gr.outputs.Textbox(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shandy77/.local/lib/python3.10/site-packages/gradio/blocks.py:922: UserWarning: api_name predict already exists, using predict_1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/shandy77/.local/lib/python3.10/site-packages/gradio/routes.py\", line 437, in run_predict\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/home/shandy77/.local/lib/python3.10/site-packages/gradio/blocks.py\", line 1352, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/home/shandy77/.local/lib/python3.10/site-packages/gradio/blocks.py\", line 1077, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/home/shandy77/.local/lib/python3.10/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/home/shandy77/.local/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2134, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/home/shandy77/.local/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/tmp/ipykernel_29774/2104295017.py\", line 2, in transcribe_speech\n",
      "    output = pipe(\n",
      "  File \"/home/shandy77/.local/lib/python3.10/site-packages/transformers/pipelines/automatic_speech_recognition.py\", line 285, in __call__\n",
      "    return super().__call__(inputs, **kwargs)\n",
      "  File \"/home/shandy77/.local/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1198, in __call__\n",
      "    return next(\n",
      "  File \"/home/shandy77/.local/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py\", line 124, in __next__\n",
      "    item = next(self.iterator)\n",
      "  File \"/home/shandy77/.local/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py\", line 266, in __next__\n",
      "    processed = self.infer(next(self.iterator), **self.params)\n",
      "  File \"/home/shandy77/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 631, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/home/shandy77/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 675, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"/home/shandy77/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 32, in fetch\n",
      "    data.append(next(self.dataset_iter))\n",
      "  File \"/home/shandy77/.local/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py\", line 183, in __next__\n",
      "    processed = next(self.subiterator)\n",
      "  File \"/home/shandy77/.local/lib/python3.10/site-packages/transformers/pipelines/automatic_speech_recognition.py\", line 410, in preprocess\n",
      "    raise ValueError(f\"We expect a numpy ndarray as input, got `{type(inputs)}`\")\n",
      "ValueError: We expect a numpy ndarray as input, got `<class 'NoneType'>`\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shandy77/.local/lib/python3.10/site-packages/gradio/routes.py\", line 437, in run_predict\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/home/shandy77/.local/lib/python3.10/site-packages/gradio/blocks.py\", line 1352, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/home/shandy77/.local/lib/python3.10/site-packages/gradio/blocks.py\", line 1077, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/home/shandy77/.local/lib/python3.10/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/home/shandy77/.local/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2134, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/home/shandy77/.local/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/tmp/ipykernel_29774/2104295017.py\", line 2, in transcribe_speech\n",
      "    output = pipe(\n",
      "  File \"/home/shandy77/.local/lib/python3.10/site-packages/transformers/pipelines/automatic_speech_recognition.py\", line 285, in __call__\n",
      "    return super().__call__(inputs, **kwargs)\n",
      "  File \"/home/shandy77/.local/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1198, in __call__\n",
      "    return next(\n",
      "  File \"/home/shandy77/.local/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py\", line 124, in __next__\n",
      "    item = next(self.iterator)\n",
      "  File \"/home/shandy77/.local/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py\", line 266, in __next__\n",
      "    processed = self.infer(next(self.iterator), **self.params)\n",
      "  File \"/home/shandy77/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 631, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/home/shandy77/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 675, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"/home/shandy77/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 32, in fetch\n",
      "    data.append(next(self.dataset_iter))\n",
      "  File \"/home/shandy77/.local/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py\", line 183, in __next__\n",
      "    processed = next(self.subiterator)\n",
      "  File \"/home/shandy77/.local/lib/python3.10/site-packages/transformers/pipelines/automatic_speech_recognition.py\", line 410, in preprocess\n",
      "    raise ValueError(f\"We expect a numpy ndarray as input, got `{type(inputs)}`\")\n",
      "ValueError: We expect a numpy ndarray as input, got `<class 'tuple'>`\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shandy77/.local/lib/python3.10/site-packages/gradio/routes.py\", line 437, in run_predict\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/home/shandy77/.local/lib/python3.10/site-packages/gradio/blocks.py\", line 1352, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/home/shandy77/.local/lib/python3.10/site-packages/gradio/blocks.py\", line 1077, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/home/shandy77/.local/lib/python3.10/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/home/shandy77/.local/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2134, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/home/shandy77/.local/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/tmp/ipykernel_29774/2104295017.py\", line 2, in transcribe_speech\n",
      "    output = pipe(\n",
      "  File \"/home/shandy77/.local/lib/python3.10/site-packages/transformers/pipelines/automatic_speech_recognition.py\", line 285, in __call__\n",
      "    return super().__call__(inputs, **kwargs)\n",
      "  File \"/home/shandy77/.local/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1198, in __call__\n",
      "    return next(\n",
      "  File \"/home/shandy77/.local/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py\", line 124, in __next__\n",
      "    item = next(self.iterator)\n",
      "  File \"/home/shandy77/.local/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py\", line 266, in __next__\n",
      "    processed = self.infer(next(self.iterator), **self.params)\n",
      "  File \"/home/shandy77/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 631, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/home/shandy77/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 675, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"/home/shandy77/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 32, in fetch\n",
      "    data.append(next(self.dataset_iter))\n",
      "  File \"/home/shandy77/.local/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py\", line 183, in __next__\n",
      "    processed = next(self.subiterator)\n",
      "  File \"/home/shandy77/.local/lib/python3.10/site-packages/transformers/pipelines/automatic_speech_recognition.py\", line 410, in preprocess\n",
      "    raise ValueError(f\"We expect a numpy ndarray as input, got `{type(inputs)}`\")\n",
      "ValueError: We expect a numpy ndarray as input, got `<class 'tuple'>`\n",
      "/home/shandy77/.local/lib/python3.10/site-packages/pydub/utils.py:198: RuntimeWarning: Couldn't find ffprobe or avprobe - defaulting to ffprobe, but may not work\n",
      "  warn(\"Couldn't find ffprobe or avprobe - defaulting to ffprobe, but may not work\", RuntimeWarning)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shandy77/.local/lib/python3.10/site-packages/gradio/processing_utils.py\", line 133, in audio_from_file\n",
      "    audio = AudioSegment.from_file(filename)\n",
      "  File \"/home/shandy77/.local/lib/python3.10/site-packages/pydub/audio_segment.py\", line 728, in from_file\n",
      "    info = mediainfo_json(orig_file, read_ahead_limit=read_ahead_limit)\n",
      "  File \"/home/shandy77/.local/lib/python3.10/site-packages/pydub/utils.py\", line 274, in mediainfo_json\n",
      "    res = Popen(command, stdin=stdin_parameter, stdout=PIPE, stderr=PIPE)\n",
      "  File \"/usr/lib/python3.10/subprocess.py\", line 971, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"/usr/lib/python3.10/subprocess.py\", line 1863, in _execute_child\n",
      "    raise child_exception_type(errno_num, err_msg, err_filename)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'ffprobe'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shandy77/.local/lib/python3.10/site-packages/gradio/routes.py\", line 437, in run_predict\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/home/shandy77/.local/lib/python3.10/site-packages/gradio/blocks.py\", line 1350, in process_api\n",
      "    inputs = self.preprocess_data(fn_index, inputs, state)\n",
      "  File \"/home/shandy77/.local/lib/python3.10/site-packages/gradio/blocks.py\", line 1200, in preprocess_data\n",
      "    processed_input.append(block.preprocess(inputs[i]))\n",
      "  File \"/home/shandy77/.local/lib/python3.10/site-packages/gradio/components/audio.py\", line 193, in preprocess\n",
      "    sample_rate, data = processing_utils.audio_from_file(\n",
      "  File \"/home/shandy77/.local/lib/python3.10/site-packages/gradio/processing_utils.py\", line 143, in audio_from_file\n",
      "    raise RuntimeError(msg) from e\n",
      "RuntimeError: Cannot load audio from file: `ffprobe` not found. Please install `ffmpeg` in your system to use non-WAV audio file formats and make sure `ffprobe` is in your PATH.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shandy77/.local/lib/python3.10/site-packages/gradio/routes.py\", line 437, in run_predict\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/home/shandy77/.local/lib/python3.10/site-packages/gradio/blocks.py\", line 1352, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/home/shandy77/.local/lib/python3.10/site-packages/gradio/blocks.py\", line 1077, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/home/shandy77/.local/lib/python3.10/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/home/shandy77/.local/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2134, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/home/shandy77/.local/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/tmp/ipykernel_29774/2104295017.py\", line 2, in transcribe_speech\n",
      "    output = pipe(\n",
      "  File \"/home/shandy77/.local/lib/python3.10/site-packages/transformers/pipelines/automatic_speech_recognition.py\", line 285, in __call__\n",
      "    return super().__call__(inputs, **kwargs)\n",
      "  File \"/home/shandy77/.local/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1198, in __call__\n",
      "    return next(\n",
      "  File \"/home/shandy77/.local/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py\", line 124, in __next__\n",
      "    item = next(self.iterator)\n",
      "  File \"/home/shandy77/.local/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py\", line 266, in __next__\n",
      "    processed = self.infer(next(self.iterator), **self.params)\n",
      "  File \"/home/shandy77/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 631, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/home/shandy77/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 675, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"/home/shandy77/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 32, in fetch\n",
      "    data.append(next(self.dataset_iter))\n",
      "  File \"/home/shandy77/.local/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py\", line 183, in __next__\n",
      "    processed = next(self.subiterator)\n",
      "  File \"/home/shandy77/.local/lib/python3.10/site-packages/transformers/pipelines/automatic_speech_recognition.py\", line 410, in preprocess\n",
      "    raise ValueError(f\"We expect a numpy ndarray as input, got `{type(inputs)}`\")\n",
      "ValueError: We expect a numpy ndarray as input, got `<class 'tuple'>`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with demo:\n",
    "    gr.TabbedInterface(\n",
    "        [mic_transcribe, file_transcribe],\n",
    "        [\"Transcribe Microphone\", \"Transcribe Audio File\"],\n",
    "    )\n",
    "\n",
    "demo.launch(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
